---
title: "Lab: Unsupervised Learning I"
author: "36-600"
output: 
  html_document:
    toc: no
    toc_float: no
    theme: spacelab
---

To answer the questions below, it will help you to refer to Sections 10.3 and 10.5 of ISLR; it might also help you to refer to your previous lab work (and, as always, to Google). 

## Question 1

Let's create a fake data frame with three rows and three columns:
```{r}
(df <- data.frame(x=1:3,y=1:3,z=1:3))

```
Computing *by hand*, what is the Euclidean distances between the fake datum of row 1 and the fake data of rows 2 and 3? And what is the Euclidean distance between the fake data of rows 2 and 3?
```
FILL ME IN WITH TEXT
Euclidean distance = √Σ(vect1i - vect2i)2 
Euclidean distances between:
row 1 and row 2: √(2-1)2 + (2-1)2 + (2-1)2 = √1+1+1 = √3 = 1.732
row 1 and row 3: √(3-1)2 + (3-1)2 + (3-1)2 = √4+4+4 = √12 = 3.464
row 2 and row 3: √(3-2)2 + (3-2)2 + (3-2)2 = √1+1+1 = √3 = 1.732
```

## Question 2

Now compute the Euclidean distances using the `dist()` function. Show the output from this function. Does that output match your hand-computed quantities? If not, go back and think re-do your calculation in Question 1. If so, then you now have a sense as to what the "distance between two data points" is, in practice. Note the appearance of the output: the distances are stored in a lower-triangular matrix.
```{r}
# FILL ME IN
dist(df, method = 'euclidean')
```

## Dataset 1

Here we import some data on stars either in, or in the same general direction as, the Draco Dwarf Galaxy.
```{r}
file.path <- "https://raw.githubusercontent.com/pefreeman/36-290/master/EXAMPLE_DATASETS/DRACO/draco_photometry.Rdata"
load(url(file.path))
df <- data.frame(ra,dec,"v"=velocity.los,log.g,"g"=mag.g,"r"=mag.r,"i"=mag.i)
rm(file.path,ra,dec,velocity.los,log.g,temperature,mag.u,mag.g,mag.r,mag.i,mag.z,metallicity,signal.noise)
```

`df` is a data frame with 2778 rows and 7 columns. See this [README file](https://github.com/pefreeman/36-290/tree/master/EXAMPLE_DATASETS/DRACO) for a full description of the data and its variables.

## Question 3

Use `dplyr` functions to filter the data frame such that it only contains values of `dec` &gt; 56, values of `ra` &lt; 264, and values of `v` between -350 and -250. (Put a space between the "<" and the "-" when you filter on the latter velocity, otherwise `R` will see the assignment operator and act accordingly.) Mutate the data frame to have g-r and r-i colors (call them `gr` and `ri`), then delete the magnitudes and `v`. (Pro tip: you can "negatively select" columns by putting minus signs in front of the column names.) Save the resulting data frame as `df.new`.
```{r}
suppressMessages(library(tidyverse))
# FILL ME IN

df.new <- df %>%
filter(dec > 56, ra < 264, v > -350, v < -250) %>%
mutate(
gr = g - r,  
ri = r - i  
) %>%
select(-v, -g, -r, -i)

head(df.new)
```

## Question 4

Use the `kmeans()` function to cluster the data in your data frame. Try different values for $k$, and finally display results for what *you* would choose as its optimal value. The default for `nstart` is 1; that should be increased to something larger...play with the values for this argument.

Install the package `ggpairs` and uncomment the library-loading line below. Then
display your results using the `ggpairs()` -> GGally (package) function; for example, use
```
ggpairs(df.new,progress=FALSE,mapping=aes(color=factor(km2.out$cluster)))
```
where `km2.out` is what I call the output from `kmeans()` for $k = 2$.

Remember to `scale()` your data when you call `kmeans()` (but not when plotting)! Also, note that `kmeans()` utilizes random sampling, so you should absolutely set a random number seed immediately before calling `kmeans()` to ensure reproducibility.
```{r}
suppressMessages(library(GGally))
# FILL ME IN
set.seed(101)

km3.out <- kmeans(scale(df.new), 3, nstart = 10)
ggpairs(df.new,progress=FALSE,mapping=aes(color=factor(km3.out$cluster)))

```

## Question 5

For your chosen $k$ value, what are the number of groups and the number of data in each group? Also, what is ratio of the between-cluster sum-of-squares to the total sum-of-squares? (This is a measure of the total variance in the data that is "explained" by clustering. Higher values [closer to 100%] are better, but beware: the larger the value of $K$, the higher the ratio is going to be: you will be getting into the realm of overfitting.) (Hint: `print()` your saved output from `kmeans()`.)

```{r}
# FILL ME IN

# Assuming km3.out is the saved kmeans output with k = 3
print(km3.out)

# 1. Number of clusters/groups
num_clusters <- length(km3.out$centers)
print(paste("Number of groups:", num_clusters))

# 2. Number of data points in each group
group_counts <- table(km3.out$cluster)
print(paste("Number of data points in each group:", group_counts))

# 3. Ratio of between-cluster sum-of-squares to total sum-of-squares
btwn_ss <- km3.out$betweenss 
total_ss <- km3.out$totss
ratio <- (btwn_ss / total_ss) * 100
print(paste("Ratio between-cluster sum-of-squares / total sum-of-squares of ratio is:", ratio))

```
```
FILL ME IN WITH TEXT
As I am choosing the value of k=3, the ratio of the between-cluster sum-of-squares to the total sum-of-squares comes out as 32.42 which does not indicate overfitting yet, however the value is high enough to indicate sufficient cluster formation for the given dataset. Testing for higher values of K indicates a convergence of the total variance where the ratio does not drastically rise towards a higher value. This could indicate sufficient cluster formation. 
```

## Question 6

Now utilize one of the methods presented in class for determinining the optimal value of $k$, while noting that some methods provide more concrete results than others.
```{r}
# FILL ME IN
library(cluster)
# Elbow method: Plot WSS for different values of k

wss <- rep(NA,10)
for ( ii in 1:10 ) {
km.out <- kmeans(scale(df.new),ii,nstart=20)
wss[ii] <- km.out$tot.withinss;
}
df.plot <- data.frame("k"=1:10,wss)
ggplot(data=df.plot,mapping=aes(x=k,y=wss)) +
geom_point(col="seagreen") +
geom_line(col="seagreen") +
ylab("Within-Cluster Sum-of-Squares")

```

## Dataset 2

## Question 7

Now input the diamonds dataset (`diamonds.csv`); download it from the `DATA` directory on `Canvas`. Once the data frame is input (be sure to use the argument
`stringsAsFactors=TRUE`, by the way), remove the columns `X`, `cut`, `color`, and `clarity` from it, and call the resulting data frame `df.new`.

```{r}
# FILL ME IN
df <- read.csv("diamonds.csv", stringsAsFactors=TRUE)
df.new <- df %>% select(-X, -cut, -color, -clarity)
```

## Question 8

The diamonds dataset is too large to input into $K$-means as is. (I have discovered this myself, to my sorrow!) So we will randomly select 1000 rows from the data frame and work with those. First, run this code (while noting that you can change the seed to whatever you like):
```
set.seed(303)
s <- sort(sample(nrow(df.new),1000))
```
Then utilize the `slice()` function from `dplyr` to select the rows recorded in the vector `s`, and save the output to `df.small`.

```{r}
# FILL ME IN
set.seed(303)
s <- sort(sample(nrow(df.new),1000))

df.small <- df.new %>% slice(s)
```

## Question 9

Now we can do $K$-means. But we'll do things a little differently this time: here, 
just run `clusGap()` directly with `K.max=8` and plot the output with `fviz_gap_stat()`. Keep that value in mind for the next question below.
```{r}
# FILL ME IN
library(cluster)
library(factoextra)

gap_stat <- clusGap(df.small, FUN = kmeans, K.max = 8)  # B is the number of bootstraps
fviz_gap_stat(gap_stat)
```

## Question 10

In Question 4, we explored using different values of $k$, and through the use of `ggpairs()` we determined that for the stellar data, $k$ should be 2 or 3. In Question 9, on the other hand, we let the gap statistic function determine the optimal value for $k$...and now here we will use `ggpairs()` to visualize the results of assuming this optimal value only. Do this below.

You need not state an answer to this question, but: does it appear that there are natural clusters in the data that were uncovered using $K$-means, or does it appear that the data were simply split into pieces? (Look at, e.g., `price` versus `carat`; that scatter plot *might* influence your thinking. Note that there not necessarily a right answer here. Interpretation, meet ambiguity.)
```{r}
# FILL ME IN

suppressMessages(library(GGally))

set.seed(101)

km.out <- kmeans(df.small, centers = 2, nstart = 20)

ggpairs(df.small,progress=FALSE,mapping=aes(color=factor(km.out$cluster)))


```

## Question 11

Read in the file `diamonds.csv` again, using `read.csv()` with the argument `stringsAsFactors=TRUE`, but do *not* remove any columns. However, do select 1000 rows again. Here we will examine these data with the $K$-prototypes algorithm.

Install the package `clustMixType`, then load it into `R` via a call to the `library()` function (uncomment the line below). Then run the function `validation_kproto()`. (To figure out how to run it, go to the help page for the function and scroll down to the examples. Use the silhouette method, set the data to `df.small` [your 1000-row subset of `df`], set `k` to 2:5, and set `nstart` to 5. Add one more argument: `verbose=FALSE`. Do not save your output to a variable...just let `validation_kproto()` output directly to your screen.) Examine the output. `indices` is the silhouette score for each value of $K$. What is the optimal number of clusters?
```{r}
# FILL ME IN
library(clustMixType)

df.new <- read.csv("diamonds.csv", stringsAsFactors=TRUE)

set.seed(101)
s <- sort(sample(nrow(df.new),1000))
df.small <- df.new %>% slice(., s)

validation_kproto(method="silhouette", data=df.small, k=2:5, nstart=5, verbose=FALSE)
```
```
FILL ME IN WITH TEXT
Optimal Number of Clusters: 2 
```

## Question 12

As a last exercise, run
```
kp.out <- kproto(df.small,[# OF CLUSTERS],nstart=5,verbose=FALSE)
```
and then plot the output: `plot(kp.out)`. For optimal results, do not run this code chunk separately, but knit the file and then look at the plots at the end of the output. You need not comment on these plots, but you should realize that they show the values of each variable for each cluster, and thus they would help you construct a story about how the clusters differ. For instance, I find that the `X` variable values differ greatly between clusters, among other variable values.
```{r}
# FILL ME IN
kp.out <- kproto(df.small, 2, nstart=5, verbose=FALSE)

plot(kp.out)
```
