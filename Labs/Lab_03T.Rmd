---
title: "Lab: Data Manipulation with dplyr"
author: "36-600"
output: 
  html_document:
    toc: no
    toc_float: no
    theme: spacelab
---

## Data

We'll begin by importing some astronomical data from the 36-290 GitHub site. These data are stored in `.Rdata` format; such data are saved via `R`'s `save()` function and loaded via `R`'s `load()` function. One wrinkle here: the data are stored on the web, so we also have to apply the `url()` function.
```{r}
load(url("https://raw.githubusercontent.com/pefreeman/36-290/master/EXAMPLE_DATASETS/BUZZARD/Buzzard_DC1.Rdata"))
set.seed(101)
s  <- sample(nrow(df),4000)
df <- df[s,-c(7:13)]
```

`df` is a data frame with 4000 rows and 7 columns; the first six are predictor variables and the seventh, `redshift`, is the response variable. Redshift is a directly observable proxy for the distance of a galaxy from the Earth. (After all, tape measures aren't going to help us here.) In a statistical learning exercise, we might try to determine a statistical model that optimally associates the predictor variables with the `redshift`.

# Questions

## Question 1

Apply the `dim()`, `nrow()`, `ncol()`, and `length()` functions to `df`, so as to build intuition about what these functions output. (Note: we are not using `dplyr` yet, just base `R` functions here.) Do you know why `length()` returns the value it does? Ask me or the TA if you do not. (But fill in an answer below regardless to reinforce the answer.)
```{r}
# FILL ME IN
dim(df)
nrow(df)
ncol(df)
length(df)
```
```
FILL ME IN WITH TEXT
The length() function returns the number of elements in an object which is the dataframe df here. It is returning the number of columns because data frames are essentially lists of columns. Hence, the length() outputs 7 corresponding to the 7 columns in the dataframe df. 
```

## Question 2

Display the names of each column of `df`.
```{r}
# FILL ME IN
colnames(df)
```

---

Time for a digression.

A *magnitude* is a logarithmic measure of brightness that is calibrated such as to be approximately zero for the brightest stars in the night sky (Sirius, Vega, etc.). Every five-unit *increase* in magnitude is a factor of 100 *decrease* in brightness. So a magnitude of 20 means the object is 100 million times fainter than a star like Vega.

Magnitudes are generally measured in particular bands. Imagine that you put a filter in front of a telescope that only lets photons of certain wavelengths pass through. You can then assess the brightness of an object at just those wavelengths. So `u` represents a magnitude determined at ultraviolet wavelengths, with `g`, `r`, and `i` representing green, red, and infrared. (The `z` and `y` are a bit further into the infrared. The names don't represent words.)

So the predictor data consists of six magnitudes spanning from the near-UV to the near-IR.

---

## Question 3

Use the base `R` `summary()` function to get a textual summary of the data frame. Do you notice anything strange? (Some values you might not expect?)
```{r}
# FILL ME IN
summary(df)
```
```
FILL ME IN WITH TEXT
From the summary, I am able to observe that some Max values are output as 99.00 which could mean they represent NA or missing values. The extremely large Max values can also dictate outliers or errors.
```

---

## dplyr

Below, we will practice using `dplyr` package functions to select rows and/or columns of a data frame. `dplyr` is part of the `tidyverse`, and is rapidly becoming the most oft-used way of transforming data. To learn more about "transformatory" functions, read through today's class notes, then through Chapter 5 of the online version of *R for Data Science* (see the syllabus for the link, or just Google for it). In short, you can

| action | function |
| ------ | -------- |
| pick features by name | `select()` |
| pick observations by value | `filter()` |
| pick observations by category | `group_by()` |
| create new features | `mutate()` |
| reorder rows | `arrange()` |
| collapse rows to summaries | `summarize()` |

A cool thing about `dplyr` is that you can use a piping operator (%&gt;%) to have the output of one function be the input to the next. And you don't have to have only `dplyr` functions within the flow; for instance, you could pipe the first two rows your data frame to head:
```{r}
suppressMessages(library(tidyverse))
head(df[,1:2])                         # base R
df %>% select(.,u,g) %>% head(.)       # dplyr 
```

Let's do a few exercises here. Be sure to tap into, e.g., StackOverflow and *R for Data Science* for any help you may need.

---

## Question 4

Grab all data for which `i` is less than 25 and `g` is greater than 22, and output in order of increasing `y`. (Remember: you combine conditions with &amp; for "and" and | for "or".) Show only the first six lines of output. Note that `head()` by default shows the first six rows of the input data frame.
```{r}
# FILL ME IN
df_filtered <- df %>%
filter(i < 25 & g > 22) %>%  
arrange(y)              

head(df_filtered)
```

## Question 5

To get a quick, textual idea of how the data are distributed: select the `g` column, pipe it to the `round()` function, then pipe the output of `round()` to `table()`. You should notice something strange about the output...the same thing you should have noticed when answering Question 3.
```{r}
# FILL ME IN
df %>%
select(g) %>%     
round() %>%     
table() 
```

---

Time for another digression. Domain scientists are not bound by the `R` convention of using `NA` when data are "not available," or missing. Sometimes the domain scientists will tell you what they use in place of `NA`, sometimes not. In those latter cases, one can usual infer the values. Astronomers for some reason love using -99, -9, 99, etc., to represent missing values. The values of 99 in the table you generated in Question 5 actually represent missing data.

We could change the values of 99 to `NA`, but here we will just filter those rows with values of 99 out of the data frame.

---

## Question 6

Use `filter()` to determine how many rows contain 99's. Since 99's can appear in different columns, you will need to combine conditions together with logical "and"s or "or"s. Note: only four of the six columns have 99's in them.
```{r}
# FILL ME IN
df_filtered <- df %>%
filter(u == 99 | g == 99 | r == 99 | i == 99 | z == 99 | y == 99 | redshift == 99)

nrow(df_filtered)
```

## Question 7

Now repeat Question 6 (in a sense), and remove all the rows that contain 99's, saving the new data frame as `df.new`. Hint that may help: the opposite of, e.g., `u==99 | g==99` is `u!=99 & g!=99`, etc. If you apply `dim()` to the new data frame, you should see that 3607 rows are retained.
```{r}
# FILL ME IN
df.new <- df %>%
filter(u != 99 & g != 99 & r != 99 & i != 99 & z != 99 & y != 99 & redshift != 99)

dim(df.new)
```

---

The data we are working with have no factor variables, so I'm going to create one on the fly:
```{r}
type <- rep("FAINT",nrow(df.new))
w <- which(df.new$i<25)
type[w] <- "BRIGHT"
type <- factor(type)
unique(type)
df.new <- cbind(type,df.new)
```
So I defined my factor variable using character strings, and then coerced the vector of strings into a factor variable with two levels. Note that by default, the levels order themselves into alphabetical order. You can override that default behavior if there is actually a natural ordering to your factor variables. See the documentation for `factor()` to see how to do that.

---

## Question 8

Use `group_by()` and `summarize()` to determine the numbers of `BRIGHT` and `FAINT` galaxies in the dataset. (If you are adventuresome: try implementing the `tally()` function instead of `summarize()`. For our purposes here, it's actually easier.)
```{r}
# FILL ME IN
#help(factor)
#df.new %>%
#group_by(type) %>%
#summarize(count = n())

df.new %>%
group_by(type) %>%
tally()
```

## Question 9

Repeat Question 8, but show the median value of the `u` magnitude instead of the numbers in each factor group. (You do need `summarize()` here; `tally()` won't help you.)
```{r}
# FILL ME IN
df.new %>%
group_by(type) %>%
summarize(median_u = median(u)) 
```

---

Time for yet another digression.

Magnitudes of galaxies at particular wavelengths are heavily influenced by two factors: physics (what is going on with the gas, dust, and stars within the galaxy itself), and distance (the further away a galaxy is, the less bright it tends to be, so the magnitude generally goes up). To attempt to mitigate (somewhat, not necessarily completely) the effect of distance, astronomers often use *colors*, which are differences in magnitude for two adjacent filters.

---

## Question 10

Use `mutate()` to define two new columns for `df.new`: `gr`, which would be the `g` magnitude minus the `r` magnitude, and `ri`, for `r` and `i`. Save your result as `df.newer`.
```{r}
# FILL ME IN
df.newer <- df.new %>%
mutate(
  gr = g - r,  
  ri = r - i   
)
#df.newer
```

## Question 11

Are the mean values of `g-r` and `r-i` roughly the same for `BRIGHT` galaxies versus `FAINT` ones? Heck if I know. Use `dplyr` functions to attempt to answer this question.
```{r}
# FILL ME IN
means_comparison <- df.newer %>%
group_by(type) %>%
summarize(
  mean_gr = mean(gr),  
  mean_ri = mean(ri)   
)

means_comparison
```

## Question 12

Let's go back to hypothesis testing for a moment. Let's extract two vectors of data from `df.newer`:
```{r}
#just use t.test(), dont code your own
gr.faint  <- df.newer$gr[df.newer$type=="FAINT"]
gr.bright <- df.newer$gr[df.newer$type=="BRIGHT"]
```
The first vector has all the `g-r` colors for faint galaxies, and the second vector has all the ones for bright galaxies. The code here *should* be familiar given what we covered in Week 01, but if you are not sure what this code is doing, call me or the TA over.

Here are the histograms for `gr.faint` and `gr.bright`:
```{r}
hist(gr.faint,prob=TRUE,main=NULL,col=alpha("firebrick",0.4),ylim=c(0,1.2),xlab="color",breaks=seq(-1.5,5.5,by=0.2))
hist(gr.bright,prob=TRUE,main=NULL,col=alpha("dodgerblue",0.4),breaks=seq(-1.5,5.5,by=0.2),add=TRUE)
```

Now: test the hypothesis that the means of the distributions that `gr.faint` and `gr.bright` are sampled from are the same. Use a two-sample $t$ test. What do you conclude? Is a two-sample $t$ test actually strictly appropriate here? How would you know? (Even if the answer is no, do the $t$ test anyway.)
```{r}
# FILL ME IN
t_test_result <- t.test(gr.faint, gr.bright)

t_test_result
```
```
FILL ME IN WITH TEXT
Here the p-value is extremely small i.e. <2.2e-16, which is very small in comparison to the significance level 0.05 (alpha), which suggests that there is a statistically significant difference between the means of gr.faint and gr.bright values.

As the alternative hypothesis suggests that the true difference in means is not equal to 0, we reject the null hypothesis which was that the means of gr.faint and gr.bright are equal.
```

