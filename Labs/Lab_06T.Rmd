---
title: "Lab: Variable Selection"
author: "36-600"
output: 
  html_document:
    toc: no
    toc_float: no
    theme: spacelab
---

## Data

We'll begin by importing the heart-disease dataset and log-transforming the response variable, `Cost`. Also, so that the dataset "plays well" with `bestglm` later, we will change the name `Cost` to `y` and put `y` last.
```{r}
suppressMessages(library(tidyverse))
df      <- read.csv("http://www.stat.cmu.edu/~pfreeman/heart_disease.csv",stringsAsFactors=TRUE)
df      <- df[,-10]
w       <- which(df$Cost > 0)
df      <- df[w,]
df$Cost <- log(df$Cost)
df$y    <- df$Cost       # create a new column on the fly
df %>% select(.,-Cost) -> df
summary(df)
```

## Question 1

Split the data into training and test sets. Call these `df.train` and `df.test`. Assume that 70% of the data will be used to train the linear regression model. Recall that
```
s <- sample(nrow(df),round(0.7*nrow(df)))
```
will randomly select the rows for training. Also recall that
```
df[s,] and df[-s,]
```
are ways of filtering the data frame into the training set and the test set, respectively. (Remember to set the random number seed!)
```{r}
# FILL ME IN WITH CODE
set.seed(101)

s <- sample(nrow(df), round(0.7 * nrow(df)))

df.train <- df[s, ]
df.test  <- df[-s, ]
```

## Question 2

Perform a multiple linear regression analysis, regressing `y` upon all the other variables, and compute the mean-squared error. Also print out the adjusted $R^2$ value; if you call the output from your linear regression function call `lm.out`, then what you'd print out is `summary(lm.out)$adj.r.squared`
```{r}
# FILL ME IN WITH CODE
# fit linear regression model on training data
lm.out <- lm(y ~ ., data = df.train)

# predict values of y for the training and test sets
train_pred <- predict(lm.out, newdata = df.train)
test_pred  <- predict(lm.out, newdata = df.test)

mse_train <- mean((df.train$y - train_pred)^2)
mse_test <- mean((df.test$y - test_pred)^2)

adj_r_squared <- summary(lm.out)$adj.r.squared

mse_train
mse_test
adj_r_squared

```

---

## Question 3

Install the `bestglm` package, if you do not have it installed already. Then load that library and use the function `bestglm()` to perform best subset selection on the training data. Do both AIC and BIC...and for each, display the best model. How many predictor variables are retained in the best models? (Don't include the intercepts.) Do the relative numbers of variables abide by your expectations? Is one model a subset of the other? (Hint: see the documentation for `bestglm()` and look at the part under "Value"...this describes the `R` object that `bestglm()` returns. The best model is included within that object. Let `bg.bic` be your output from `bestglm()` for BIC, and `bg.aic` be the output for AIC. If the documentation states that `xx` is the element of the output that contains the best model, then simply print, e.g., `bg.bic$xx`. In the end, what gets returned from functions is either a vector [not here!] or a list. If you need to know the names of the elements of the list, type, e.g., `names(bg.bic)`. Doing that here might be helpful: the element with the best model might jump out at you!)
```{r}
# FILL ME IN WITH CODE
# install.packages("bestglm")
library(bestglm)

# bic
bg.bic <- bestglm(df.train, IC = "BIC")
# aic
bg.aic <- bestglm(df.train, IC = "AIC")

print("best model bic")
print(bg.bic$BestModel)

print("best model aic")
print(bg.aic$BestModel)

# subtract 1 to not include the intercept from the count
# coef returns all coefficients including the intercept
# intercept is not a predictor variable so we subtract it
bic_vars <- length(coef(bg.bic$BestModel)) - 1
aic_vars <- length(coef(bg.aic$BestModel)) - 1

print("How many predictor variables are retained in the best models")
print(bic_vars)
print(aic_vars)

print("Is one model a subset of the other")
bic_model_terms <- names(coef(bg.bic$BestModel))[-1] # remove intercept
aic_model_terms <- names(coef(bg.aic$BestModel))[-1]

is_subset <- all(bic_model_terms %in% aic_model_terms)
print(bic_model_terms)
print(aic_model_terms)
print(is_subset)
```
```
FILL ME IN WITH TEXT

How many predictor variables are retained in the best models?
- 4 in BIC, 5 in AIC

Do the relative numbers of variables abide by your expectations?
- BIC normally prefers simpler models that means fewer variables whereas AIC typically includes more variables to fit the data more closely. Here as we can see that BIC has 4 variables and AIC has 5, that is more than BIC, thus the relative numbers match our expectations.

Is one model a subset of the other?
- Yes, BIC is a subset of AIC
```

## Question 4

The output of `bestglm()` contains, as you saw above, a best model. According to the documentation for `bestglm()`, this list element is "[a]n lm-object representing the best fitted algorithm." That means you can pass it to `predict()` in order to generate predicted response values (where the response is in the `y` column of your data frames). Given this information: generate mean-squared error values for the BIC- and AIC-selected models. Are these values larger or smaller than the value you got for linear regression?
```{r}
# FILL ME IN WITH CODE
bic_pred <- predict(bg.bic$BestModel, newdata = df.test)
mse_bic <- mean((df.test$y - bic_pred)^2)

aic_pred <- predict(bg.aic$BestModel, newdata = df.test)
mse_aic <- mean((df.test$y - aic_pred)^2)

print("BIC MSE, AIC MSE and Linear Regression Model MSE")
print(mse_bic)
print(mse_aic)
print(mse_test)
```
```
FILL ME WITH TEXT

Are these values larger or smaller than the value you got for linear regression?

- BIC MSE is slightly larger than the linear regression model MSE, which suggests that the full model has sacrificed a small amount of predictive accuracy compared to the full model and therefore it has a slightly worse performing model in comparison to the Linear Regression model.

- AIC MSE is slightly smaller than Linear regression which means that subset selection improved the models prediction performance by removing less useful variables therefore improving fit.

```

---

Here is code that allows you to visualize, e.g., the BIC as a function of number of variables. Note that in this example, `bg.bic` is the output of `bestglm(...,IC="BIC")`. This is just FYI: if you ever use variable selection in practice, you might find this visualizer useful.

```{r}
bic    <- bg.bic$Subsets["BIC"]
df.bic <- data.frame("p"=1:ncol(df.train)-1,"BIC"=bic[,1])

g <- ggplot(data=df.bic,mapping=aes(x=p,y=BIC)) + 
       geom_point(size=1.5,color="blue") + 
       geom_line(color="blue") + 
       ylim(min(bic),min(bic+100))  # a quick and dirty way to try to hone in on the right range to see minimum
suppressWarnings(print(g)) # a way to get around pesky ggplot warnings
```

---

## Question 5

Run the `summary()` function with the best BIC model from above. This produces output akin to that of the output from summarizing a linear model (e.g., one output by `lm()`). What is the adjusted $R^2$ value? What does the value imply about the quality of the linear fit with the best subset of variables?
```{r}
# FILL ME IN WITH CODE
bic_summary <- summary(bg.bic$BestModel)
adj_r2_bic <- bic_summary$adj.r.squared
print(bic_summary)
print(adj_r2_bic)
```
```
FILL ME IN WITH TEXT
Value of 0.595 suggests that the best subset model selected by BIC explains about 59.5% of variance with response variable y. The fit is moderate, sitting between 0 and 1. Also means that the remaining 40.5% of data has variance that is unexplained by the model, which can be due to factors such as measurement error, or inherent randomness in data. Value of 0.595 tells that the model is capturing a good amount of data variability but it can still be further improved with respect to its predictive power.
```
