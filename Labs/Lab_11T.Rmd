---
title: "Lab: Nonlinear Regression"
author: "36-600"
output: 
  html_document:
    toc: no
    toc_float: no
    theme: spacelab
---

```{r}
suppressMessages(library(tidyverse))
```

## Data

We'll begin by simulating a dataset from a nonlinear curve:
```{r}
set.seed(555)
x <- -5:5
y <- 0.1*x^3 - 0.5*x + 2.1 + rnorm(length(x),mean=0,sd=0.5*(1+abs(x)))
e <- 0.5*(1+abs(x))

df <- data.frame("x"=x,"y"=y,"e"=e)

suppressMessages(library(tidyverse))
ggplot(data=df,mapping=aes(x=x,y=y)) +
  geom_errorbar(aes(ymin=y-e, ymax=y+e), width=.1,color="blue") +
  geom_point(color="firebrick")
```

# Questions

## Question 1

Implement a (weighted) global cubic polynomial regression model in a similar manner to that implemented in the notes; namely, that means learn the model, run predict to determine the regression line, plot the data with the regression line superimposed, show the coefficients, and compute the mean-squared error. Like we did(n't do) in the notes, do not split the data into training and test datasets.
```{r echo=FALSE,fig.width=4,fig.height=4,fig.align="center"}
# REPLACE ME WITH CODE

# Set up weights using the error values (inverse-variance weighting)
weights <- 1 / (df$e)^2

# Fit a weighted cubic polynomial model
model <- lm(y ~ poly(x, 3, raw=TRUE), data=df, weights=weights)

# Display model coefficients
coefficients <- summary(model)$coefficients[, 1]
print("Model Coefficients:")
print(coefficients)

# Predict values for plotting the regression line
x_range <- seq(min(df$x), max(df$x), length.out=100)
y_pred <- predict(model, newdata=data.frame(x=x_range))
line_data <- data.frame(x=x_range, y=y_pred)  # Create a new data frame for the line

# Compute the mean-squared error
predicted_values <- predict(model, newdata=df)
mse <- mean((df$y - predicted_values)^2)
print(paste("Mean Squared Error:", round(mse, 4)))

# Plotting
ggplot(data=df, mapping=aes(x=x, y=y)) +
  geom_errorbar(aes(ymin=y-e, ymax=y+e), width=0.1, color="blue") +
  geom_point(color="firebrick") +
  geom_line(data=line_data, aes(x=x, y=y), color="darkgreen", size=1) +  # Use the line data here
  labs(title="Weighted Cubic Polynomial Regression",
       subtitle=paste("MSE:", round(mse, 4)),
       x="x", y="y")



```

## Question 2

Repeat Q1, but utilizing a regression splines model. Assume four degrees of freedom.
```{r echo=FALSE,fig.width=4,fig.height=4,fig.align="center"}
# REPLACE ME WITH CODE
# Load necessary libraries
suppressMessages(library(splines))

# Set up weights using the error values (inverse-variance weighting)
weights <- 1 / (df$e)^2

# Fit a regression spline model with four degrees of freedom
model_spline <- lm(y ~ bs(x, df=4), data=df, weights=weights)

# Display model coefficients
coefficients_spline <- summary(model_spline)$coefficients[, 1]
print("Spline Model Coefficients:")
print(coefficients_spline)

# Predict values for plotting the regression spline line
x_range <- seq(min(df$x), max(df$x), length.out=100)
y_pred_spline <- predict(model_spline, newdata=data.frame(x=x_range))
line_data_spline <- data.frame(x=x_range, y=y_pred_spline)  # Create a new data frame for the line

# Compute the mean-squared error
predicted_values_spline <- predict(model_spline, newdata=df)
mse_spline <- mean((df$y - predicted_values_spline)^2)
print(paste("Mean Squared Error (Spline):", round(mse_spline, 4)))

# Plotting
ggplot(data=df, mapping=aes(x=x, y=y)) +
  geom_errorbar(aes(ymin=y-e, ymax=y+e), width=0.1, color="blue") +
  geom_point(color="firebrick") +
  geom_line(data=line_data_spline, aes(x=x, y=y), color="purple", size=1) +  # Use the line data here
  labs(title="Weighted Regression Spline Model",
       subtitle=paste("MSE:", round(mse_spline, 4)),
       x="x", y="y")

```

## Question 3

Repeat Q1, but with a smoothing spline model. Note that you may get a "surprising" result.
```{r echo=FALSE,fig.width=4,fig.height=4,fig.align="center"}
# REPLACE ME WITH CODE

# Fit a smoothing spline model with cross-validation to select the best smoothing parameter
model_smoothing_spline <- smooth.spline(df$x, y=df$y, w=1/(df$e)^2, cv=TRUE)

# Display smoothing parameter (lambda)
lambda <- model_smoothing_spline$lambda
print(paste("Smoothing Parameter (Lambda):", round(lambda, 6)))

# Predict values for plotting the smoothing spline line
x_range <- seq(min(df$x), max(df$x), length.out=100)
y_pred_smoothing_spline <- predict(model_smoothing_spline, x=x_range)$y
line_data_smoothing <- data.frame(x=x_range, y=y_pred_smoothing_spline)

# Compute the mean-squared error
predicted_values_smoothing <- predict(model_smoothing_spline, x=df$x)$y
mse_smoothing_spline <- mean((df$y - predicted_values_smoothing)^2)
print(paste("Mean Squared Error (Smoothing Spline):", round(mse_smoothing_spline, 4)))

# Plotting
ggplot(data=df, mapping=aes(x=x, y=y)) +
  geom_errorbar(aes(ymin=y-e, ymax=y+e), width=0.1, color="blue") +
  geom_point(color="firebrick") +
  geom_line(data=line_data_smoothing, aes(x=x, y=y), color="darkorange", size=1) +  # Use the line data here
  labs(title="Weighted Smoothing Spline Model",
       subtitle=paste("MSE:", round(mse_smoothing_spline, 4)),
       x="x", y="y")

```

## Question 4

Repeat Q1, but with a local polynomial regression model. Assume a `span` of 0.6.
```{r}
# REPLACE ME WITH CODE

# Fit a local polynomial regression model with specified span
model_loess <- loess(y ~ x, data=df, weights=1/(df$e)^2, span=0.6)

# Predict values for plotting the local polynomial regression line
x_range <- seq(min(df$x), max(df$x), length.out=100)
y_pred_loess <- predict(model_loess, newdata=data.frame(x=x_range))
line_data_loess <- data.frame(x=x_range, y=y_pred_loess)

# Compute the mean-squared error
predicted_values_loess <- predict(model_loess, newdata=df)
mse_loess <- mean((df$y - predicted_values_loess)^2)
print(paste("Mean Squared Error (Local Polynomial):", round(mse_loess, 4)))

# Plotting
ggplot(data=df, mapping=aes(x=x, y=y)) +
  geom_errorbar(aes(ymin=y-e, ymax=y+e), width=0.1, color="blue") +
  geom_point(color="firebrick") +
  geom_line(data=line_data_loess, aes(x=x, y=y), color="darkcyan", size=1) +  # Use the line data here
  labs(title="Weighted Local Polynomial Regression Model",
       subtitle=paste("MSE:", round(mse_loess, 4)),
       x="x", y="y")

```

## Question 5

Redo the plot in Q4, but let's add a one-standard-error confidence band. You can do this by running the first line, then adding the last two lines onto your `ggplot()` call:
```
p <- predict(lpr.out,se=TRUE)

+ geom_line(mapping=aes(x=lpr.out$x,y=lpr.out$fitted+p$se),color="[your color]",linetype="dashed")
+ geom_line(mapping=aes(x=lpr.out$x,y=lpr.out$fitted-p$se),color="[your color]",linetype="dashed")
```
What does the band actually mean? Because it's a one-standard-error band, it means that for any given $x$, there is an approximately 68% chance that the band overlaps the true underlying function value. This is a rough statement, though, given the correlation between neighboring data points (i.e., the lack of independence between $y_{i-1}$, $y_i$, and $y_{i+1}$, etc.). Just think of the band as a notion of how uncertain your fitted curve is at each $x$: is the band thin, or wide? Note that the bands get wider as we get to either end of the data: this is an expected feature, not a bug. There's fewer data within the span at either end, so the fitted function is that much more uncertain.
```{r}
# REPLACE ME WITH CODE

# Fit a local polynomial regression model with specified span
model_loess <- loess(y ~ x, data=df, weights=1/(df$e)^2, span=0.6)

# Generate predictions with standard errors
p <- predict(model_loess, newdata=data.frame(x=df$x), se=TRUE)

# Create data for the confidence bands
line_data_loess <- data.frame(
  x = df$x,
  fitted = p$fit,
  upper = p$fit + p$se.fit,
  lower = p$fit - p$se.fit
)

# Predict values for a smooth line
x_range <- seq(min(df$x), max(df$x), length.out=100)
y_pred_loess <- predict(model_loess, newdata=data.frame(x=x_range))
smooth_line_data <- data.frame(x=x_range, y=y_pred_loess)

# Compute the mean-squared error
predicted_values_loess <- predict(model_loess, newdata=df)
mse_loess <- mean((df$y - predicted_values_loess)^2)
print(paste("Mean Squared Error (Local Polynomial):", round(mse_loess, 4)))

# Plotting with the one-standard-error confidence band
ggplot(data=df, mapping=aes(x=x, y=y)) +
  geom_errorbar(aes(ymin=y-e, ymax=y+e), width=0.1, color="blue") +
  geom_point(color="firebrick") +
  geom_line(data=smooth_line_data, aes(x=x, y=y), color="darkcyan", size=1) +  # Fitted line
  geom_line(data=line_data_loess, mapping=aes(x=x, y=upper), color="darkcyan", linetype="dashed") +  # Upper confidence band
  geom_line(data=line_data_loess, mapping=aes(x=x, y=lower), color="darkcyan", linetype="dashed") +  # Lower confidence band
  labs(title="Weighted Local Polynomial Regression with One-Standard-Error Band",
       subtitle=paste("MSE:", round(mse_loess, 4)),
       x="x", y="y")

```
