---
title: "Lab: Hypothesis Testing"
author: "36-600"
output:
  html_document:
    toc: false
    theme: spacelab
---

## Question 1

In an experiment, you sample $n = 10$ data from a normal distribution with unknown
mean $\mu$ and *known* standard deviation $\sigma$, and you wish to perform a
lower-tail hypothesis test. Your null hypothesis is that $\mu_o = 6$, the sample mean is $\bar{X} = 5$, the known standard deviation is $\sigma$ = 4, and $\alpha = 0.05$.
What is the $p$-value for this test? Do you reject or fail to reject the null?

What do you need to know to carry out this test?

1. The sampling distribution for $\bar{X}$, under the null, is a normal with mean
$\mu_o$ and standard deviation $\sigma/\sqrt{n}$.
2. The appropriate cumulative distribution function is thus `pnorm()` (whose
arguments can be looked up by, e.g., typing `help(pnorm)` at the prompt in
the `Console` pane). (Note that the first argument, `q`, is here the observed statistic value.)

Go back to the notes to determine the appropriate equation for the $p$-value for
a lower-tail test.
```{r}
# FILL ME IN
n<-10
mu_o<-6
sample_mean<-5
sigma<-4
alpha<-0.05
z<-(sample_mean-mu_o)/(sigma/(sqrt(n)))
z
pt_lowertail<-pnorm(z)
pt_lowertail

if (pt_lowertail > alpha){
  print("we fail to reject the null hypothesis")
}else{
  print("we reject the null hypothesis")
}
```
```
FILL ME IN WITH TEXT
we fail to reject the null hypothesis
```

## Question 2

What if we had decided to do a two-tail test instead? What would the $p$-value be?
```{r}
# FILL ME IN
pt_twotail <- 2*pnorm(z)
pt_twotail
if (pt_twotail > alpha){
  print("we fail to reject the null hypothesis")
}else{
  print("we reject the null hypothesis")
}

```

## Question 3

Now assume that instead of 10 data, you have 100 data. What is the $p$-value now,
for a lower-tail test? Do we reject or fail to reject the null?
```{r}
# FILL ME IN
n<-100
mu_o<-6
sample_mean<-5
sigma<-4
alpha<-0.05
z<-(sample_mean-mu_o)/(sigma/(sqrt(n)))
z
pt_lowertail<-pnorm(z)
pt_lowertail

if (pt_lowertail > alpha){
  print("we fail to reject the null hypothesis")
}else{
  print("we reject the null hypothesis")
}
```
```
FILL ME IN WITH TEXT
we reject the null hypothesis
```

## Question 4

A concept that we did not cover in the notes is the concept of test power.

The power of a hypothesis test is the rate at which we would reject a stated 
null hypothesis given a arbitrary true value of $\mu$. For instance, above we
assume a null value of $\mu_o = 6$...if we use that null value but it turns out
that $\mu = 5.5$ instead, we would reject the null more than $100\alpha$-percent
of the time. But how would we quantify this?

We need to do two things. First, we need to determine the value of $\bar{X}$ that
lies on the boundary of the rejection region, i.e., the value such that if $\bar{X}$
is smaller, we reject the null, and if $\bar{X}$ is larger, we fail to reject
the null. In general, this boundary is determined using an inverse cumulative
distribution function:
```
boundary <- [inverse cdf]([arguments])
```
For the lower-tail test with a normal sampling distribution, the function
is `qnorm()` and the arguments are `alpha`, `mean=mu.o`, and `sd=sigma/sqrt(n)`.
Call the output `x.bar.rr` (with `rr` standing for "rejection region").

Second, to compute the power, we would use the exact same code that we use to
compute the $p$-value, except we would swap out $\bar{X}$ for $\bar{X}_{RR}$ and
we would swap out $\mu_o$ for $\mu$. (Assume $n = 10$.) You should observe a
power of 0.45: 45% of the time that we replicate the experiment, we would reject
the null, if our null is $\mu_o = 6$ and if the true value is actually $\mu = 5.5$.
(To increase the test power, we can (a) increase the sample size $n$,
and/or (b) increase the difference between $\mu_o$ and $\mu$.)
```{r}
# FILL ME IN
n<- 10
mu_o<- 6
sigma<- 4
alpha<- 0.05

x_bar_rr<- qnorm(alpha,mean=mu_o,sd=(sigma/sqrt(n)))
x_bar_rr

mu<- 5.5
power_test<- pnorm(x_bar_rr,mean=mu,sd=(sigma/sqrt(n)))
power_test

```

## Question 5

Now assume that we have the following data:
```{r}
set.seed(101)
n <- 15
x <- rgamma(n,shape=3,scale=3)
```
Here, $n = 15$ and the true mean is $3 \cdot 3 = 9$. However, we don't know the
mean, and we wish to conduct an upper-tail test of $\mu_o = 7.5$. How would we
do this?

First, we need to assume a distribution. We can assume that our data are sampled
from a normal distribution. But is this a valid supposition?

To determine this, we can use the *Shapiro-Wilk test*. For this test, the null
hypothesis is that the data are normally distributed; if we fail to reject the
null, then it means that the assumption of normality is valid and that we can
use, e.g., `pnorm()` to compute the $p$-value if the standard deviation is
known or `pt()` if it is not.

Pass the data into the `shapiro.test()` function below and make a conclusion as
to whether or not we can assume the data are normally distributed.
```{r}
# FILL ME IN
shapiro_wilk_test<-shapiro.test(x)
shapiro_wilk_test

sample_mean<-mean(x)
std_dev<- sd(x)
mu_o<-7.5
z<-(sample_mean - mu_o)/(std_dev/sqrt(n))
z
pnorm(z)
pt_uppertail<- 1 - pnorm(z)
pt_uppertail

```
```
FILL ME IN WITH TEXT
Since the p-value (0.2656) is greater than the typical significance level of (alpha = 0.05), we fail to reject the null hypothesis, this means that the assumption of normality is valid. However, there is not enough data to conclude that the data are normally distributed.
```

## Question 6

In the previous question, you should have come to the conclusion that the data
are plausibly normally distributed. Carry out the hypothesis test described in
that question utilizing code given in the lecture notes and determine whether or
not we reject the null hypothesis that $\mu_o = 7.5$.
```{r}
# FILL ME IN
sample_mean<-mean(x)
std_dev<- sd(x)
mu_o<-7.5
z<-(sample_mean - mu_o)/(std_dev/sqrt(n))
z
pnorm(z)
pt_uppertail<- 1 - pnorm(z)
pt_uppertail

alpha <- 0.05  
if (pt_uppertail < alpha) {
  print("we reject the null hypothesis")
}else{
  print("we fail to reject the null hypothesis")
}
```
```
FILL ME IN WITH TEXT
Since the pt_uppertail is greater than the typical significance level of (alpha = 0.05), we fail to reject the null hypothesis, this means that there is not enough evidence to say the mean is greater than 7.5.
```

## Question 7

Let's confirm the result in Question 6 using the `t.test()` function.

Look at the documentation for the `t.test()` function by typing `help(t.test)`
at the prompt in the `Console` pane. The relevant arguments are `x` (which 
represents our data), `alternative` (which here should be set to `"greater"`),
and `mu` (which is the null value: 7.5).
```{r}
# FILL ME IN
#help(t.test)
t_test<- t.test(x, alternative = "greater", mu = 7.5)
t_test
```

## Question 8

Let's use the same data to perform a lower-tail test of the null hypothesis that 
the population variance, $\sigma^2$, is equal to 25.

What statistic should we use? It stands to reason that we should use the
*sample variance* $S^2$, which we compute using the function `var()`.

What is the sampling distribution for this statistic? That's complicated to
write down, but it turns out that's OK, since we can write down that 
$(n-1)S^2/\sigma^2$ is, under the null, sampled from a chi-square distribution 
for $n-1$ degrees of freedom instead. (Hence you'd utilize `pchisq()` to compute
the $p$-value.)

That should be sufficient information for you to be able to compute the $p$-value.
Call us over if you need a little help. My $p$-value is 0.029.
```{r}
# FILL ME IN
sigma_sq<-25
sample_var<- var(x)
chi_sq <- (n-1)*sample_var / sigma_sq
chi_sq
pt_lowertail <- pchisq(chi_sq, df= n-1)
pt_lowertail
```

## Question 9

Let's confirm the result in Question 8 using the `varTest()` function from the
`EnvStats` package.

First, install the package if it is not installed already. To do that, go to 
the `Packages` pane, click on `Install`, and type in `EnvStats`. (Then 
uncomment the `library(EnvStats)` function call below.)

Second, run `varTest()`, passing in the data along with the arguments
`alternative="less"` and `sigma.squared=25`. Note that if you don't want to
see all the output but rather just the $p$-value, you can tack `$p.value` onto
the end of the function call...this says that of all the elements in the list
that is output by `varTest()`, you just want to print out the value of the
element `p.value`.

```{r}
suppressMessages(library(EnvStats))

# FILL ME IN
#install.packages("EnvStats")
var_test_result<- varTest(x, alternative="less", sigma.squared=25)

var_test_result$p.value
```

## Question 10

Let's do one last hypothesis test. Let's assume that we sample $n = 5$ data
from an exponential distribution with mean value $\mu = 2$:
```{r}
set.seed(202)
n <- 5
x <- rexp(n,rate=1/2)
```
Now, we don't know that the mean is 2, and our goal is to perform an upper-tail
test of the null hypothesis $\mu = \mu_o = 1$, assuming $\alpha = 0.05$.

To carry out this test, it suffices to know that

1. the statistic we will use is $\bar{X}$ (or `mean(x)`); and
2. the sampling distribution for $\bar{X}$ is, under the null, a gamma distribution with
`shape` parameter `n` and `scale` parameter `1/n` (hence we utilize `pgamma()`).

If your $p$-value is 0.008, you've gotten the correct answer!
```{r}
# FILL ME IN
alpha<- 0.05
sample_mean <- mean(x)
sample_mean
pt_uppertail <- 1 - pgamma(sample_mean, shape = n, scale = 1/n)
pt_uppertail
```

# Question 11

In the notes, we mention that if the null is true, the distribution for the $p$-value is uniform, with values between 0 and 1. Let's demonstrate that here.
```{r}
set.seed(404)
p <- rep(NA,1000)       # set aside storage for 1000 p-values
n <- 100                # assume a sample size of 100 for each experiment
for ( ii in 1:1000 ) {  # do 1000 experiments
  x     <- rnorm(n)                                 # sample 100 values from a standard normal
  p[ii] <- pnorm(mean(x),mean=0,sd=sd(x)/sqrt(n))   # do a lower-tail test
}
hist(p,col="seagreen")  # make a histogram of p-values
```

What do you observe? Is it plausible to conclude that when the null hypothesis is true, the $p$-values are distributed uniformly?
```
FILL ME IN WITH TEXT
Observation: The histogram of p-values is roughly flat indicating a uniform distribution between 0 and 1. This is expected under the null hypothesis as values are evenly spread. Thereby concluding, that the null hypothesis is TRUE.
```

## Question 12

Here we will utilize the Kolmogorov-Smirnov test to test the null hypothesis
that the $p$-values generated in Question 11 are uniformly distributed between
0 and 1.

Look at the documentation for the function `ks.test()`. Let's look at 
three arguments here:

- `x`: as we will do a one-sample KS test here, that would be the variable $p$ from the last question;
- `y`: as we will test to see if the $p$-values are uniformly distributed, we will provide the name of a cdf function here...that would be `"punif"`; and
- `alternative`: you need not specify this, as we will do a default two-sided test, but you should realize this is where you specify the range of alternative hypotheses.

So, now: test whether the $p$-values derived in Question 11 are uniformly distributed. Print the $p$-value for the KS test and state a qualitative conclusion.
```{r}
# FILL ME IN
#help("ks.test")
ks.test(p, "punif")
```
```
FILL ME IN WITH TEXT
Based on the Kolmogorov-Smirnov test, the p-value is greater than alpha (0.05), which means that the distribution is uniform. Thus, it supports the null hypothesis that the p-values are uniformly distributed between 0 and 1.
```

As a parting note, let's look at what the KS test is actually doing.
```{r}
plot(ecdf(p),xlab="p",ylab=expression(F[P]*"(p)"),main=NULL)
lines(c(-1,0,1,2),c(0,0,1,1),col="red")
```

The KS test statistic is the maximum vertical distance from the red line (the cumulative distribution function for a Uniform(0,1) distribution) to the black curve (the empirical cumulative distribution function for the $p$-values). That distance is a random variable whose distribution, under the null, is the Kolmogorov distribution. If the observed maximum distance is sufficiently large, then we would reject the null hypothesis and conclude that the $p$-values are *not* uniformly distributed. 

**It is good to reiterate here that hypothesis testing is all about specifying a test statistic (computed from the data), its sampling distribution under the null, and determining the probability of observing the test statistic or a value more extreme (which is the $p$-value). Everything else is test-specific detail.**

